///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2025, 2026 Oracle and/or its affiliates.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

///////////////////////////////////////////////////////////////////////////////

= LangChain4j
:description: Helidon LangChain4j Integration
:keywords: helidon, AI, LangChain4j, LC4J
:feature-name: LangChain4j Integration
:rootdir: {docdir}/../../..

include::{rootdir}/includes/se.adoc[]

== Contents

* <<Overview, Overview>>
* <<Features, Features>>
* <<Maven Coordinates, Maven Coordinates>>
* <<General Concepts, General Concepts>>
    ** <<Providers, LangChain4j Providers>>
    ** <<Supplier Factory, Supplier Factory>>
* <<Configuration, Configuration>>
    ** <<Configuration Migration Guide, Configuration Migration Guide>>
* <<Declarative AI, Declarative AI>>
    ** <<AI Services, AI Services>>
    ** <<Agents, Agents>>
    ** <<Agentic Workflow, Agentic Workflow>>
* <<Tools (Callback Functions), Tools (Callback Functions)>>
* <<Observability (ChatModelListeners), Observability (ChatModelListeners)>>
* <<Additional Information, Additional Information>>

== Overview
// Referenced from the guide
// tag::overview[]
https://github.com/langchain4j/langchain4j[LangChain4j] is a Java framework for building AI-powered applications using Large Language Models (LLMs). It provides seamless integration with multiple LLM providers, including OpenAI, Cohere, Hugging Face, and others. Key features include AI Services and Agents for model interaction, support for Retrieval-Augmented Generation (RAG) to enhance responses with external data, and tools for working with embeddings and knowledge retrieval.

Helidon provides a LangChain4j integration module that simplifies the use of LangChain4j in Helidon applications.

[NOTE]
LangChain4j integration is a preview feature. The APIs shown here are subject to change. These APIs will be finalized in a future release of Helidon.
// end::overview[]

== Features

- *Integration with Helidon Inject*
+
Automatically creates and registers selected LangChain4j components in the Helidon service registry based on configuration.

- *Integration with CDI*
+
Thanks to the *Helidon Inject to CDI Bridge*, LangChain4j components can be used in CDI environments, including Helidon MP applications.


- *Declarative AI Services and Agents*
+
Supports https://docs.langchain4j.dev/tutorials/ai-services/[LangChain4j's AI Services] and https://docs.langchain4j.dev/tutorials/agents/[Agents] within the declarative programming model, allowing for clean, easy-to-manage code structures.

include::{rootdir}/includes/dependencies.adoc[]

[source,xml]
----
<dependency>
    <groupId>io.helidon.integrations.langchain4j</groupId>
    <artifactId>helidon-integrations-langchain4j</artifactId>
</dependency>
----
// Referenced from the guide
// tag::annotation-processors[]

Include the following annotation processor in the `<build><plugins>` section of `pom.xml`:

[source,xml]
----
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-compiler-plugin</artifactId>
    <configuration>
        <annotationProcessorPaths>
            <path>
                <groupId>io.helidon.bundles</groupId>
                <artifactId>helidon-bundles-apt</artifactId>
                <version>${helidon.version}</version>
            </path>
        </annotationProcessorPaths>
    </configuration>
</plugin>
----
// end::annotation-processors[]

Some features of the integration may require adding other dependencies. Check the corresponding sections for additional information.


== General Concepts

LangChain4j integration in Helidon is orchestrating LangChain4j AiServices and Agents as named singleton declarative service beans.
Singletons can be referenced by their names and created either by configuration-driven <<Providers,providers>> or manually as any other
declarative Helidon service bean, with <<Supplier Factory,supplier factory>>, for example.

=== Providers

Helidon LangChain4j providers are extensions that enable integration with a range of AI vendors.
Each provider is identified and configured using a unique key.
You can define multiple named models, reuse provider-level configuration, or override it with model-specific properties.

image::lc4j/model-provider-config.svg[Provider and model configuration merge, align="center"]

Once you have configured named model, like `cheaper-model` on the example above, you can reference it from
AiServices and Agents via `@Ai.ChatModel` or `@Ai.StreamingChatModel` annotations by its name:

[source,java]
----
@Ai.Service
@Ai.ChatModel("cheaper-model") //<1>
public interface ChefAiService {
    @SystemMessage("You are an expert in the food preparation.")
    @UserMessage("""
            Provide a short step-by-step instructions how to prepare: {{foodName}}
            """)
    String cookingInstructions(@V("foodName") String foodName);
}
----
<1> Custom name selected in the model configuration above

Providers available out of the box:

[cols="7,8,11"]
|===
| Provider                                         | Provider Key                      | Description
| LangChain4j content retriever                    | `lc4j-content-retriever`          | Built-in content-retriever
| LangChain4j in-memory embedding store            | `lc4j-in-memory`                  | Built-in in-memory store
| LangChain4j in-process models                    | `lc4j-in-process`                 | Local https://docs.langchain4j.dev/integrations/embedding-models/in-process[in-process models]
| xref:open-ai.adoc[*Open AI*]                     | `open-ai`                         | OpenAI and OpenAI compatible models providers
| xref:oci-genai.adoc[*OCI GenAI*]                 | `oci-gen-ai`, `oci-gen-ai-cohere` | Oracle Cloud Infrastructure GenAI models
| xref:jlama.adoc[*Jlama*]                         | `jlama`                           | Local inference with selected https://github.com/tjake/Jlama[Jlama] models
| xref:gemini.adoc[*Google Gemini*]                | `google-gemini`                   | Google Gemini hosted models
| xref:ollama.adoc[*Ollama*]                       | `ollama`                          | Ollama hosted models support
| xref:cohere.adoc[*Cohere*]                       | `cohere`                          | Cohere hosted models
| xref:oracle.adoc[*Oracle Embedding Store*]       | `oracle`                          | Oracle Database as an embedding store
| xref:coherence.adoc[*Coherence Embedding Store*] | `coherence`                       | Coherence as embedding and chat memory store
| xref:mock.adoc[*Mock*]                           | `helidon-mock`                    | Mockable chat model for deterministic testing
|===


NOTE: Missing your favorite AI vendor already supported by LangChain4j? You can generate your own Helidon integration with our
xref:codegen-provider.adoc[*LangChain4j Model Provider Generator*] or use the supplier factory!

=== Supplier Factory

Supplier Factory provides another way to create and register LangChain4j components. It is useful for creating components
that are not yet natively supported by the integration, such as ChatModels, Embedding Models, Embedding Stores or Content Retrievers.
This method is not limited to LangChain4j and suitable for creating and registering other classes.

The example below demonstrates a supplier factory for `MistralAiChatModel`.

[source,java]
----
@Service.Singleton
@Service.Named("custom-chat-model") //<1>
class ChatModelFactory implements Supplier<ChatModel> {
    @Override
    public ChatModel get() {
        return MistralAiChatModel.builder()
                .apiKey(ApiKeys.MISTRALAI_API_KEY)
                .modelName(MistralAiChatModelName.MISTRAL_SMALL_LATEST)
                .build();
    }
}
----
<1> Custom name of the resulting declarative service bean referencable from Ai Services or Agents

NOTE: Supplier factories can be *standalone* or *static inner* classes.

To use such a manually created model, reference it by name.

[source,java]
----
@Ai.Service
@Ai.ChatModel("custom-chat-model") //<1>
public interface ChefAiService {
    @SystemMessage("You are an expert in the food preparation.")
    @UserMessage("""
            Provide a short step-by-step instructions how to prepare: {{foodName}}
            """)
    String cookingInstructions(@V("foodName") String foodName);
}
----
<1> Custom name selected in the supplier factory above


== Configuration

Helidon LangChain4j uses a unified configuration that separates *providers* (shared configuration) from *named components* such as models, embedding stores, content retrievers, services, and agents.
Components are enabled by default; add `enabled: false` to disable a component entry explicitly.

Key concepts:

* *Providers*: Shared configuration per provider key, with optional defaults for model/embedding store creation.
* *Named components*: Models, embedding stores, content retrievers, services, and agents are named entries and become named singleton beans.
* *Multiple models per provider*: You can configure multiple models for a single provider by adding multiple entries under `langchain4j.models`.
* *Overrides*: Component configuration overrides provider defaults during merge.

[source,yaml]
----
langchain4j:
  providers:
    foo-bar-provider-name:
      # config common for all models/embedding stores referencing this provider

  models:
    foo-bar-model-name:
      # model-specific config is merged with provider config; model is named singleton bean 'foo-bar-model-name'
      provider: foo-bar-provider-name

  services:
    foo-bar-service-name:
      # overrides service annotation setup
      streaming-chat-model: foo-bar-model-name

  agents:
    foo-bar-agent-name:
      # overrides agent annotation setup
      chat-model: foo-bar-model-name

  embedding-stores:
    foo-bar-embedding-store-name:
      provider: foo-bar-provider-name

  content-retrievers:
    foo-bar-content-retriever-name:
      provider: helidon
      type: embedding-store-content-retriever

  mcp-clients:
    foo-bar-mcp-server:
      uri: http://foo-bar-mcp-server:1234/foo
----

=== Configuration Migration Guide

WARNING: The configuration format has changed in version 4.4 in a backward-incompatible way

Changes in 4.4:

* Provider configuration now lives under `langchain4j.providers`.
* Models and embedding stores are named entries under `langchain4j.models` and `langchain4j.embedding-stores` etc.
* Providers and components are enabled by default (set `enabled: false` to disable).
* You can configure multiple models of the same type by adding multiple entries under `langchain4j.models`.

==== Example Migration

Pre 4.4 configuration:

[source,yaml]
----
langchain4j:
  open-ai:
    # Models were referenced by the provider name, in this case 'open-ai'
    chat-model:
      enabled: true
      api-key: "${OPEN_AI_TOKEN}"
      model-name: "gpt-4o-mini"
----

New configuration as documented in <<Configuration>>:

[source,yaml]
----
langchain4j:
  models:
    # Custom model names are required, to be referencable, in this case 'cheaper-model'
    cheaper-model:
      provider: open-ai
      api-key: "${OPEN_AI_TOKEN}"
      model-name: "gpt-4o-mini"
----

== Declarative AI

LangChain4j AI Services provide a declarative and type-safe way to define AI-powered functionality. It allows combining chat models, retrieval-augmented generation (RAG), chat memory, and other building blocks to create sophisticated AI-driven workflows. Read more about it in https://docs.langchain4j.dev/tutorials/ai-services[LangChain4j documentation].

Helidon LangChain4j integration provides two declarative approaches for using AI:

- AI Services
- Agents and Agentic Workflows

Services and Agents are typically Java interfaces annotated with LangChain4j and Helidon annotations,
 the resulting implementation is created by LangChain4j runtime and wired together by xref:{rootdir}/se/injection/injection.adoc[Helidon as singleton declarative
service beans]. You can access those anywhere in Helidon with `Services.get(FooBarAiService.class)` or inject it in another
service bean as constructor parameter. Thanks to the CDI bridge, you can inject AI Services and Agents even to Helidon MP CDI beans.

Both AI Services and Agents can be configured with the following Helidon annotations:

[cols="1,5"]
|===
| Annotation               | Description
| `Ai.ChatModel`           | Specifies the name of a service in the service registry that implements `ChatModel` to be used in the annotated AI Service. Mutually exclusive with `Ai.StreamingChatModel`.
| `Ai.StreamingChatModel`  | Specifies the name of a service in the service registry that implements `StreamingChatModel` to use in the annotated Ai Service. Mutually exclusive with `Ai.ChatModel`.
| `Ai.ChatMemoryProvider`  | Specifies the name of a service in the service registry that implements `ChatMemoryProvider` to use in the annotated Ai Service.
| `Ai.ModerationModel`     | Specifies the name of a service in the service registry that implements `ModerationModel` to use in the annotated Ai Service.
| `Ai.ContentRetriever`    | Specifies the name of a service in the service registry that implements `ContentRetriever` to use in the annotated Ai Service. Mutually exclusive with `Ai.RetrievalAugmentor`.
| `Ai.RetrievalAugmentor`  | Specifies the name of a service in the service registry that implements `RetrievalAugmentor` to use in the annotated Ai Service. Mutually exclusive with `Ai.ContentRetriever`.
| `Ai.ToolProvider`        | Specifies the name of a service in the service registry that implements `ToolProvider` to use in the annotated Ai Service. Mutually exclusive with `Ai.McpClients`.
| `Ai.Tools`               | Specifies the classes with tools. In case a singleton service bean of the same type exists, its instance is supplied.
| `Ai.McpClients`          | Specifies the name/s of a `McpClient` in the service registry that implements `ToolProvider` to use in the annotated Ai Service. `McpToolProvider` is created from these clients. Mutually exclusive with `Ai.ToolProvider`.
|===

=== AI Services

AI Service is defined by a Java interface. It's a pure LangChain4j component. Refer to https://docs.langchain4j.dev/tutorials/ai-services[LangChain4j documentation] to read more details about it.

Helidon's LangChain4j integration provides a specialized set of annotations for creating, configuring, and using LangChain4j AI Services in Helidon applications.

To create an AI Service define an interface and annotate it with `@AI.Service`.

[source, java]
----
@Ai.Service
public interface ChatAiService {
    String chat(String question);
}
----

NOTE: AI services can be named using `@Ai.Service("name")`. Named AI services can be configured under `langchain4j.services`, where values in configuration override annotation values.

=== Agents

LangChain4j agents are AI services enhanced for agentic workflows. In Helidon, each agent is a named declarative singleton service. Agent configuration can be set using annotations and overridden by Helidon config under `langchain4j.agents`.

To define a named agent create an interface using `@Ai.Agent` and annotate the method with LangChain4j `@Agent`:

[source,java]
----
@Ai.Agent("cli-expert")
@Ai.ChatModel("custom-model-name")
@Ai.McpClients("cli-tools-mcp-server")
public interface CliExpert {

    @UserMessage("""
            You are a command line expert helping users with Helidon CLI.
            Provide a short step-by-step answer to the request: {{request}}
            and always include the exact CLI command on its own line.
            """)
    @Agent(value = "Helidon CLI specialist", outputKey = "response")
    String answer(@V("request") String request);
}
----

Agents can be configured or overridden using `langchain4j.agents.<agent-name>` entries, for example to replace a chat model or adjust an output key.

==== Agentic Workflow

Helidon supports LangChain4j declarative agentic workflows such as sequence and conditional agents. Each subagent remains a named Helidon service, and agentic systems can be composed using declarative annotations like `@SequenceAgent` and `@ConditionalAgent`.

[source,java]
----
@Ai.Agent("helidon-expert")
public interface HelidonExpertAgent {

    @SequenceAgent(outputKey = "response", subAgents = {
            FlavorClassifierAgent.class,
            FlavorRouterAgent.class
    })
    String ask(@V("request") String request);
}
----

== Tools (Callback Functions)

In LangChain4j, tools are callback functions that the language model can invoke during a conversation to perform specific tasks, retrieve information, or execute external logic. These tools extend the model's capabilities beyond simple text generation, allowing it to dynamically interact with external systems. For instance, a tool might query a database, call an external API, or perform calculations. Based on user input, the model can decide to call a tool, interpret its response, and incorporate it into the conversation for a more context-aware and multi-step interaction.

To expose a method in a Helidon service as a tool, annotate it with the `@Tool` annotation from `dev.langchain4j.agent.tool.Tool`, as shown in the example below:

[source,java]
----
@Service.Singleton
public class OrderService {

    @Tool("Get order details for specified order number")
    public Order getOrderDetails(String orderNumber) {
        // ...
    }
}
----

NOTE: If you are using Helidon MP, to enable `@Tool`-annotated methods in CDI beans, you must annotate the CDI bean with the `@Ai.Tool` qualifier.

For more details, read the https://docs.langchain4j.dev/tutorials/tools#high-level-tool-api[LangChain4j Documentation on Tools].

== Observability (ChatModelListeners)

While LangChain4j doesn't provide Observability out-of-box, it provides for user to supplement it using `ChatModelListener`. For more details, read the https://docs.langchain4j.dev/tutorials/observability/[LangChain4j Documentation on Observability].

Helidon provides `MetricsChatModelListener` which generates metrics that follow the https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/gen-ai-metrics.md[OpenTelemetry Semantic Conventions for GenAI Metrics v1.36.0]. This is done out-of-box for Chat API calls.

== Additional Information

* https://docs.langchain4j.dev/[LangChain4j documentation]
* Components Reference
** xref:codegen-provider.adoc[Code generated Lc4j Provider]

