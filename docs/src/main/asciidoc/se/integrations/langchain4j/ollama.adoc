///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2025 Oracle and/or its affiliates.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

///////////////////////////////////////////////////////////////////////////////

= LangChain4J Ollama Provider
:description: LangChain4J Ollama
:keywords: helidon, AI, LangChain4J, LC4J, Ollama
:feature-name: LangChain4J Integration
:rootdir: {docdir}/../../..

include::{rootdir}/includes/se.adoc[]

== Contents

* <<Overview, Overview>>
* <<Maven Coordinates, Maven Coordinates>>
* <<Components, Components>>
** <<OllamaChatModel, OllamaChatModel>>
** <<OllamaEmbeddingModel, OllamaEmbeddingModel>>
** <<OllamaLanguageModel, OllamaLanguageModel>>
** <<OllamaStreamingChatModel, OllamaStreamingChatModel>>
* <<Additional Information, Additional Information>>

== Overview

This module adds support for selected https://ollama.ai/[Ollama] models.

== Maven Coordinates

In addition to the xref:langchain4j.adoc#maven-coordinates[Helidon integration with LangChain4J core dependencies], you must add the following:

[source,xml]
----
<dependency>
    <groupId>io.helidon.integrations.langchain4j.providers</groupId>
    <artifactId>helidon-integrations-langchain4j-providers-ollama</artifactId>
</dependency>
----

== Components

=== OllamaChatModel

To automatically create and add `OllamaChatModel` to the service registry add the following lines to `application.yaml`:

[source,yaml]
----
langchain4j:
  ollama:
    chat-model:
      enabled: true
----

If `enabled` is set to `false`, the configuration is ignored, and the component is not created.

Full list of configuration properties:

[cols="3,3a,5a"]

|===
|Key |Type |Description

|`base-url` |string |The base URL for the Ollama API. If not present, the default value supplied from LangChain4J is used.
|`enabled` |boolean |If set to false (default), the component will not be available even if configured.
|`format` |string |Specifies the structure or style of the text produced by the model, such as plain text, JSON, or a custom format.
|`log-requests` |boolean |Whether to log API requests.
|`log-responses` |boolean |Whether to log API responses.
|`max-retries` |integer |The maximum number of retries for failed API requests.
|`model-name` |string |The model name to use.
|`num-predict` |int |Length of the output generated by the model.
|`repeat-penalty` |double |The penalty applied to repeated tokens during text generation. Higher values discourage the model from generating the same token multiple times, promoting more varied and natural output. A value of `1.0` applies no penalty (default behavior), while values greater than `1.0` reduce the likelihood of repetition. Excessively high values may overly penalize common phrases, leading to unnatural results.
|`seed` |int |The seed for the random number generator used by the model.
|`stop` |string[] |List of sequences where the API will stop generating further tokens.
|`temperature` |double |Sampling temperature to use, between 0 and 2. Higher values make the output more random, while lower values make it more focused and deterministic.
|`timeout` |duration |The timeout setting for API requests. See https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-[here] for the format.
|`top-k` |int |Limits the token pool to the `topK` highest-probability tokens, controlling the balance between deterministic and diverse outputs. A smaller `topK` (e.g., 1) results in deterministic output, while a larger value (e.g., 50) allows for more variability and creativity.
|`top-p` |double |Nucleus sampling value, where the model considers the results of the tokens with top_p probability mass.

|===

=== OllamaEmbeddingModel

To automatically create and add `OllamaEmbeddingModel` to the service registry add the following lines to `application.yaml`:

[source,yaml]
----
langchain4j:
  ollama:
    embedding-model:
      enabled: true
----

If `enabled` is set to `false`, the configuration is ignored, and the component is not created.

Full list of configuration properties:

[cols="3,3a,5a"]

|===
|Key |Type |Description

|`base-url` |string |The base URL for the Ollama API. If not present, the default value supplied from LangChain4J is used.
|`enabled` |boolean |If set to false (default), the component will not be available even if configured.
|`log-requests` |boolean |Whether to log API requests.
|`log-responses` |boolean |Whether to log API responses.
|`max-retries` |integer |The maximum number of retries for failed API requests.
|`model-name` |string |The model name to use.
|`timeout` |duration |The timeout setting for API requests. See https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-[here] for the format.

|===


=== OllamaLanguageModel

To automatically create and add `OllamaLanguageModel` to the service registry add the following lines to `application.yaml`:

[source,yaml]
----
langchain4j:
  ollama:
    language-model:
      enabled: true
----

If `enabled` is set to `false`, the configuration is ignored, and the component is not created.

Full list of configuration properties:

[cols="3,3a,5a"]

|===
|Key |Type |Description

|`base-url` |string |The base URL for the Ollama API. If not present, the default value supplied from LangChain4J is used.
|`enabled` |boolean |If set to false (default), the component will not be available even if configured.
|`format` |string |Specifies the structure or style of the text produced by the model, such as plain text, JSON, or a custom format.
|`log-requests` |boolean |Whether to log API requests.
|`log-responses` |boolean |Whether to log API responses.
|`max-retries` |integer |The maximum number of retries for failed API requests.
|`model-name` |string |The model name to use.
|`num-predict` |int |Length of the output generated by the model.
|`repeat-penalty` |double |The penalty applied to repeated tokens during text generation. Higher values discourage the model from generating the same token multiple times, promoting more varied and natural output. A value of `1.0` applies no penalty (default behavior), while values greater than `1.0` reduce the likelihood of repetition. Excessively high values may overly penalize common phrases, leading to unnatural results.
|`seed` |int |The seed for the random number generator used by the model.
|`stop` |string[] |List of sequences where the API will stop generating further tokens.
|`temperature` |double |Sampling temperature to use, between 0 and 2. Higher values make the output more random, while lower values make it more focused and deterministic.
|`timeout` |duration |The timeout setting for API requests. See https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-[here] for the format.
|`top-k` |int |Limits the token pool to the `topK` highest-probability tokens, controlling the balance between deterministic and diverse outputs. A smaller `topK` (e.g., 1) results in deterministic output, while a larger value (e.g., 50) allows for more variability and creativity.
|`top-p` |double |Nucleus sampling value, where the model considers the results of the tokens with top_p probability mass.

|===


=== OllamaStreamingChatModel

To automatically create and add `OllamaStreamingChatModel` to the service registry add the following lines to `application.yaml`:

[source,yaml]
----
langchain4j:
  ollama:
    streaming-chat-model:
      enabled: true
----

If `enabled` is set to `false`, the configuration is ignored, and the component is not created.

Full list of configuration properties:

[cols="3,3a,5a"]

|===
|Key |Type |Description

|`base-url` |string |The base URL for the Ollama API. If not present, the default value supplied from LangChain4J is used.
|`enabled` |boolean |If set to false (default), the component will not be available even if configured.
|`format` |string |Specifies the structure or style of the text produced by the model, such as plain text, JSON, or a custom format.
|`log-requests` |boolean |Whether to log API requests.
|`log-responses` |boolean |Whether to log API responses.
|`max-retries` |integer |The maximum number of retries for failed API requests.
|`model-name` |string |The model name to use.
|`num-predict` |int |Length of the output generated by the model.
|`repeat-penalty` |double |The penalty applied to repeated tokens during text generation. Higher values discourage the model from generating the same token multiple times, promoting more varied and natural output. A value of `1.0` applies no penalty (default behavior), while values greater than `1.0` reduce the likelihood of repetition. Excessively high values may overly penalize common phrases, leading to unnatural results.
|`seed` |int |The seed for the random number generator used by the model.
|`stop` |string[] |List of sequences where the API will stop generating further tokens.
|`temperature` |double |Sampling temperature to use, between 0 and 2. Higher values make the output more random, while lower values make it more focused and deterministic.
|`timeout` |duration |The timeout setting for API requests. See https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-[here] for the format.
|`top-k` |int |Limits the token pool to the `topK` highest-probability tokens, controlling the balance between deterministic and diverse outputs. A smaller `topK` (e.g., 1) results in deterministic output, while a larger value (e.g., 50) allows for more variability and creativity.
|`top-p` |double |Nucleus sampling value, where the model considers the results of the tokens with top_p probability mass.

|===

== Additional Information

* xref:langchain4j.adoc[LangChain4J Integration]
* https://docs.langchain4j.dev/integrations/language-models/ollama/[LangChain4J Ollama Documentation]
* https://ollama.ai/[Ollama Website]